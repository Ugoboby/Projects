---
header-includes: \usepackage{dsfont}\usepackage{setspace}\usepackage{amsmath}\usepackage[utf8]{inputenc}\usepackage[T1]{fontenc}\usepackage[french]{babel}
output: pdf_document
editor_options: 
  chunk_output_type: console
---

\hypersetup{pdfborder=0 0 0}

\thispagestyle{empty}
\begin{center}
\LARGE \textbf{Université de Bordeaux} \normalsize\\ \vfill
\Large Apprentissage Suppervisée \\
Master 2 - Modélisation Statistique et Stochastique\\ \vspace{0.5\baselineskip}
\normalsize \vfill

\rule{0.95\textwidth}{2pt}\vspace{0.5\baselineskip}\\
 \Huge \textbf{TP1 : Classification
 Méthodes des k plus proches voisins
 Règles de classifications de Bayes}\\
\rule{0.95\textwidth}{2pt}\\ \vfill\normalsize
\Large \textbf{Cours disposée par Mme Marie Chavent} \\ \vfill


\begin{tabular}{ll}
\textsc{Cheick et Lacauste}
\end{tabular}
\end{center}
\normalsize
\newpage
\setcounter{page}{1}

\newpage

**Objectifs du TP :**
\vspace{5mm}

Exercice 1 :
\begin{itemize}
  \item{Appliquer les knn sur un exemple.}
  \item{Découvrir les notions de taux d’erreur d’apprentissage et de taux d’erreur test.}
  \item{Découvrir l’importance du choix du paramètre k.}
\end{itemize}
Exercice 2 :
\begin{itemize}
  \item{Simuler des données à partir d’un mélange de lois Gaussiennes.}
  \item{Appliquer de manière empirique la règle de Bayes avec la méthode knn.}
  \item{Evaluer le taux d’erreur test avec plusieurs découpages.}
  \item{Comparer ce taux d’erreur test au taux d’erreur de la règle du "max à priori".}
  \item{Découvrir les notions de VP (vrais positifs), FP (faux positifs), VN (vrais négatifs), FN (faux négatifs) en classification binaire et les notions de sensibilité (TVP), de spécificité (TVN), de précision et de F-mesure.}
  \item{Découvrir le problème des données déséquilibrées.}
  \item{Appliquer de manière empirique la règle de Bayes avec une matrice de coûts.}
  \item{Evaluer la F-mesure par validation croisée 5-folds.}
\end{itemize}
Exercice 3 :
\begin{itemize}
  \item{Apprendre à choisir (calibrer) le paramètre k.}
  \item{Comprendre la notion d’échantillon de validation et son utilisation pour choisir un paramètre.}
\end{itemize}
Exercice 4 :

  - Appliquer la méthode knn sur des données réelles et évaluer les performances de cette méthode pour ces données.

Exercice 5 :
\begin{itemize}
  \item{Constuire une score dans le cas binaire avec la méthode knn.}
  \item{Construire la courbe ROC de ce score et évaluer le critère AUC sur des données test.}
  \item{Choisir un seuil pour constuire une règle de classification.}
\end{itemize}

\newpage

# Exercice 1

### 1) Chargement des données.

```{r eval=FALSE}
train <-read.table("synth_train.txt", header=TRUE)
Xtrain <- train[,-1]
Ytrain <- train$y
```

### 2) Représentation Graphique du jeu de données avec la variable à prédire Y

  On démarre par une première analyse graphique de notre jeu de donnée en faisant apparaitre les deux diffèrentes classes prise par la variables Y en  ajustant à l'oeil avec une couleur et une forme diffèrente (obtenue comme un paramètre de la foncion plot repectiveent par les arguments 'col' et 'pch')

```{r echo=FALSE}
train <-read.table("synth_train.txt", header=TRUE)
Xtrain <- train[,-1]
Ytrain <- train$y
plot(Xtrain, pch=Ytrain, col=Ytrain)
legend("topleft", legend=c("classe1", "classe2"), pch=1:2, col=1:2)
```

\newpage
## Méthodes des k-nn

  Dans ce paragraphe on va utiliser la méthodes ders k-plus proches voisins comme règle de classification. Nous étudierons ensuite en détail quel parrallèle nous pouvons effectuer avec les critères de Bayes pour la classification

### 3) Avec le package


```{r}
library(class)
Xtest <- matrix(c(0,0,-2,1), nrow=2, byrow=TRUE)
pred <- knn(Xtrain,Xtest, Ytrain, k=30, prob=TRUE)
```

```{r echo=FALSE}
pred
```

la valeur retournée correspond a la prédiction obtenus par la méthode knn issus de la fonction knn du package class, pour les deux points que l'on cherche à prédire.
La seconde valeur est la probailité obtenue lors de cette classification. 
Dans le premier cas (O;O) les 30 plus proches voisins sont dans la seconde classe on obtient donc une proba de 1
Dans le second cas (-2;1) une majorité de 57% des 30 plus proches voisins sont dans la classe 1

### 4) Avec une programation de la fonction k-nn

 Dans ce nouveau cas nous nous proposons de coder nous même une fonction pour sortir les probabilités de reclasifiès un groupe d'individus suivant la classe de leur k voisins. Tout d'abord j'ai essayer d'utiliser la fonction dist() de R{stats} pour retourner une matrice de distance, mais j'ai du me résoudre à imbriquer deux boucles avec un calcul de distance euclidiennes classiques : 
 $$dist(X,Y) = \sqrt{\sum(x_{i}-y_{i})^2}$$
 On récupère ensuite les voisins grâce à la fonction order() de R{base} qui nous donne l'indice des valeurs obtenus par tri descendant. Ensuite on récupère la classe des voisins pour faire le calcul de la proba de classification on copie la fonction de R{class} pour ressortir à chaque fois la probabilité d'appartenir à la classe prédites, i.e. celle qui a le plus de voisins.
 
```{r eval=FALSE}
kppv <- function(Xtrain, Xtest, Ytrain, k){
  pred <- rep(NA, nrow(Xtest))
  proba <- rep(NA, nrow(Xtest))
  vectdist <- rep(NA, nrow(Xtest))
  for (i in 1:nrow(Xtest)){
    vectdist <- rep(NA, nrow(Xtrain))
    for (j in 1 : nrow(Xtrain)){
      vectdist[j] = sqrt(sum((Xtest[i,]-Xtrain[j,])^2))
    }
  vY <- Ytrain[order(vectdist)[1:k]]
    if (sum(vY == 1) > sum(vY == 2)){
      pred[i] <- 1
      proba[i] <- sum(vY==1)/k
    }
    else {
      pred[i] <- 2
      proba[i] <- sum(vY==2)/k
    }
  }
  return(list("On prédit la(es) valeur(s):", pred, "Avec la(es) probabilitée(s) :",proba))
}
```

On vérifie que l'on retrouve bien le même résultat qu'avec la fonction R{class}

```{r echo=FALSE}
kppv <- function(Xtrain, Xtest, Ytrain, k){
  pred <- rep(NA, nrow(Xtest))
  proba <- rep(NA, nrow(Xtest))
  vectdist <- rep(NA, nrow(Xtest))
  for (i in 1:nrow(Xtest)){
    vectdist <- rep(NA, nrow(Xtrain))
    for (j in 1 : nrow(Xtrain)){
      vectdist[j] = sqrt(sum((Xtest[i,]-Xtrain[j,])^2))
    }
  vY <- Ytrain[order(vectdist)[1:k]]
    if (sum(vY == 1) > sum(vY == 2)){
      pred[i] <- 1
      proba[i] <- sum(vY==1)/k
    }
    else {
      pred[i] <- 2
      proba[i] <- sum(vY==2)/k
    }
  }
  return(list("On prédit la(es) valeur(s):", pred, "Avec la(es) probabilitée(s) :",proba))
}

kppv(Xtrain, Xtest, Ytrain, k = 30)
```


## 5) Calcul du taux d'erreur d'apprentissage

Un calcul qui peut s'avérer utile lors de la construction de notre méthode de classification est de calculer le taux d'erreur de classification sur le jeu de donnée d'apprentissage. Ce calcul s'éffectue en appliquant notre modéle de classification aux données d'apprentissages et de compter le nombre d'erreur de classifications que l'on pondérera par l'ensemble des données pour obtenir un taux.
Ce calcul se justifie surtout pour nous éviter de conclure un modèle qui serait "sur-appris", en effet un taux d'erreur vraiment trop faible, nous indiquerait que le modèle "utilise" surement trop la spécificité du jeu de donnée sur lequelle il s'est construit.
Pour notre modèles de classification avec le méthode des k plus proches voisins à k=30 voisins, on obtient le taux d'erreur d'apprentissage suivant :

```{r echo=FALSE}
pred_train <- knn(Xtrain, Xtrain, Ytrain, 30)
sum(pred_train!=Ytrain)/ length(Ytrain)# taux d'erreur d'apprentissage
```
La valeur de 0.1 ici nous donne un taux satisfaisant pour se servir du modèle.

## 6) Avec les données d'entraînement

On utilise la même méthode de calcul que pour les données d'apprentissages, mise à part qu'ici le jeu de donnée n'a pas servi lors de la construction du modèle. Ce calcul nous permet d'observer également un problème de sur-apprentissage, en effet un taux trop different du taux d'erreur d'appretissage nous indiquerais une spécificté du jeu de donnée dans la construction du modèle. On se sert plus volontier de ce taux pour évaluer la rigueur d'un modéle car on effectue une estimation sur un jeu de donnée qui n'a pas servi dans la construction du modèle.
On trouve avec nos données test un taux d'erreur de :
```{r echo =FALSE}
test <- read.table(file="synth_test.txt", header=TRUE)
Xtest <- test[,-1]
Ytest <- test$y
pred_test <- knn(Xtrain,Xtest,Ytrain,30)
sum(pred_test!=Ytest)/length(Ytest)# taux d'erreur test
```
Un taux d'erreur à 17% est acceptable dans un modèle de classification, on est proche du taux d'erreur d'apprentissage, on ne peut donc pas conclure à l'existence d'un problème de sur-apprentissage, et avec un taux de reclassement suppérieur à 70%, on se situe au-dessus de la règle de classification basique, celle qui classe toutes les prédictions dans la classe majoritaire, ici ce serait la seconde classe qui nous donerais un taux 

## 7) Calcul des taux d'erreur d'apprentissage et de test (ou entrainement) pour différentes valeurs de k

### Pour k=15
#### Taux d'erreur d'apprentissage


```{r echo=FALSE}
pred_train <- knn(Xtrain, Xtrain, Ytrain, 15)
sum(pred_train!=Ytrain)/ length(Ytrain)# taux d'erreur d'apprentissage
```

#### Taux d'erreur d'entraînement
```{r echo=FALSE}
pred_test <- knn(Xtrain,Xtest,Ytrain,15)
sum(pred_test!=Ytest)/length(Ytest)# taux d'erreur test
```



### Pour k=1
#### Taux d'erreur d'apprentissage

```{r echo=FALSE}
pred_train <- knn(Xtrain, Xtrain, Ytrain, 1)
sum(pred_train!=Ytrain)/ length(Ytrain)# taux d'erreur d'apprentissage
```
Ici, il est évident que si l'on prend seulement un voisin toute les classe du jeu d'apprentissage vont être bien prédite, en effet le voisin choisi sera le point à prédire dont on connait déjà la classe.

#### Taux d'erreur test
```{r echo=FALSE}
pred_test <- knn(Xtrain,Xtest,Ytrain,1)
sum(pred_test!=Ytest)/length(Ytest)# taux d'erreur test
```
Il nous donne quand même une erreur de test non nul, c'est l'expression du sur-apprentissage.

## 8) Constuction de la grille de points

On se propose maintenant d'evaluer la méthode des k-nn de manière graphique pour observer ll'influence du choix du parammètre k dans la construction de ce que l'on appelle frontière de décision, on va faier en sorte de construire une grille de points reprenants la zone des individus du jeu de données et appliquer à chaque points de la grille la méthode des k-nn en clasification.
On s'y prend de la manière suivante :

```{r eval=FALSE}
a <- seq(from=min(Xtrain$x1), to=max(Xtrain$x1), length.out=100)
b <- seq(from=min(Xtrain$x2), to=max(Xtrain$x2), length.out=100)
grille <- NULL
for(i in a){
  grille <- rbind(grille, cbind(i,b))
}
colnames(grille) <- c("x1","x2")
```


### Pour k=30 voisins

```{r echo=FALSE}
a <- seq(from=min(Xtrain$x1), to=max(Xtrain$x1), length.out=100)
b <- seq(from=min(Xtrain$x2), to=max(Xtrain$x2), length.out=100)
grille <- NULL
for(i in a){
  grille <- rbind(grille, cbind(i,b))
}
colnames(grille) <- c("x1","x2")

pred_grille <- knn(Xtrain, grille, Ytrain, 30)
plot(grille, pch = 20, col = pred_grille, cex = 0.5, main="Frontière de décision pour k=30 voisins")
points(Xtrain, pch=Ytrain, col=Ytrain)
legend("topleft", legend= c("classe 1", "classe 2"), pch=1:2, col=1:2, bg="white")
```
On obtient donc quelque chose de très visuel, on observe une "frontière de décision" qui délimite les zones rouge et noir. on remarque aussi que certains points noirs, ce retrouvent dans la zone rouge, alors qu'aucun point rouge ne se retrouve dans la zone noir, ce qui s'ignifie que l'on a peut être pas le meilleur k possible pour reclassifier les individus. C'est erreur de classification sont celles qui apparaissent dans le calcul du taux d'erreur d'apprentissage. On peut dire qu'avec un choix de k voisins trop grands, on favorise trop la classe dominante.
Essayons donc avec un nouveau choix pour le paramètre k.

### Pour k=15 voisins

```{r echo = FALSE}
a <- seq(from=min(Xtrain$x1), to=max(Xtrain$x1), length.out=100)
b <- seq(from=min(Xtrain$x2), to=max(Xtrain$x2), length.out=100)
grille <- NULL
for(i in a){
  grille <- rbind(grille, cbind(i,b))
}
colnames(grille) <- c("x1","x2")
pred_grille <- knn(Xtrain, grille, Ytrain, 15)
plot(grille, pch = 20, col = pred_grille, cex = 0.5, main="Frontière de décision pour k=15 voisins")
points(Xtrain, pch=Ytrain, col=Ytrain)
legend("topleft", legend= c("classe 1", "classe 2"), pch=1:2, col=1:2, bg="white")
```
On compare donc ce résultat à celui qui a été trouvé precedemment, on peut tout d'abord se rendre compte qu'il y a un point rouge classifier dans la zone noir ce qui nous dit que toutes les erreurs d'apprentisage, ne sont pas liés à un seul type de mauvais reclassement. De plus la frontière de décision est un peu plus "simple", pour k=30 voisins, on avait une frontière de décision hyperbolique, ici elle est plus assimilable à une droite. La simplification de la règle de classification est un avantage dans l'uilisation de celui-ci pour la prédiction.
Réduisons à nouveau le nombre k de voisins.

### Pour k=1 voisin

```{r echo=FALSE}
pred_grille <- knn(Xtrain, grille, Ytrain, 1)
plot(grille, pch = 20, col = pred_grille, cex = 0.5, main="Frontière de décision pour k=1 voisin")
points(Xtrain, pch=Ytrain, col=Ytrain)
legend("topleft", legend= c("classe 1", "classe 2"), pch=1:2, col=1:2, bg="white")
```

Ici on a établi notre règle de décision sur le plus proche voisin, on a donc décidé de classé chaque individu dans la classe de son voisin le plus proche, par cette méthode on observe bien que chaque point de l'ensemble d'apprentissage a bien était classé dans sa bonne classe, chaque point de couleur se trouve dans la zone de couleur correspondante. On remarque cependant que la frontière de décision a une forme vraiment compliqué, et de plus un élement rouge au milieu de plusieurs noirs nous donne une zone de classification en rouge qui forme un disque dans le plan de reclassification en noir. Cette paramétrisation nous permet de visualiser un problème de sur-apprentissage sur le jeu de donnée.

\newpage

# Exercice 2

Pour ce nouvel exercice, nous nous proposons de travailler sur un jeu de donnée, que nous aurions simulé au préalable. Pour se faire on utilise un mélange de loi gaussienne obtenue avec la fonction R{mvtnorm}, qui s'appelle rmvnorm()
Le mélange gaussien satisfait l'équation suivante : 

$$ 0.2 \times \mathbb{N}(\mu_1, \Sigma) + 0.8 \times \mathbb{N}(\mu_2, \Sigma)$$ 

## 1. On considère d’abord que le coût de mauvaise classification dans la classe 1 est égal au coût de mauvaise classification dans la classe 2 (matrice de coûts 0-1).

```{r}
#Simulation d'une comninaison linéaire de variables gaussiennes
library(mvtnorm)
n <- 1000
set.seed(10)
U =runif(n)
X <- matrix(NA,n,2)
Y <- rep(NA,n)
for(i in 1:n) {
  if(U[i] < .2) {
    X[i,] <- rmvnorm(1,c(0,0),diag(c(1,1)))
    Y[i] <- 1
  } else {
    X[i,] <- rmvnorm(1,c(2,0),diag(c(1,1)))
    Y[i] <- 2
  }
}
```


```{r echo=FALSE}
plot(X, pch=Y, col=Y)
legend("topleft", legend=c("classe1", "classe2"), pch=1:2, col=1:2)
barplot(table(Y)/n, ylim = c(0,1), main = "Proportion des Classes")
```



### Comment la méthode knn applique de manière empirique la règle de Bayes ? 
  La méthode knn applique de manière empirique la règle de Bayes avec une matrice de coûts 1-0, en estimant la probabilité à postériori, à partir des proportions de chaque classe parmis les k voisins.
### Quelle est la mesure de performance minimisée ?
La mesure de performance minimisée est le taux d'erreur téorique.

### Evaluation du taux d'erreur de la méthode knn avec 10 voisins
```{r eval=FALSE}
#découpage du jeu de données en un jeu d'apprentissage et un jeu de test
set.seed(10)
tr <- sample(1:n,800)
Xtrain <- X[tr,]
Ytrain <- Y[tr]
Xtest <- X[-tr,]
Ytest <- Y[-tr]
```

Ici on parle du taux d'erreur test, lorsque l'on ne précise pas explicitement le terme de taux d'erreur, comme expliquer plus haut, ce taux est calculé en effectuer une classification du jeu de donnée test par la règle de classification obtenue sur les données d'apprentissage. Et on calcul ce taux comme suit :
$$\frac{\text{Nombre d'Erreur de Classification}}{\text{Nombre de Prédictions au Total}} $$

Taux d'erreur test de la méthode à $k=10$ voisins :

```{r echo=FALSE}
predi = knn(Xtrain, Xtest, Ytrain, 10)
taux_erreur = sum(predi != Ytest)/length(Ytest)
taux_erreur
```

### En enlevant le set.seed

On se propose de recalculer le traitement en enlevant la commande set.seed

```{r}
tr <- sample(1:n,800)
Xtrain <- X[tr,]
Ytrain <- Y[tr]
Xtest <- X[-tr,]
Ytest <- Y[-tr]
```

On trouve alors un taux d'erreur égal à :

```{r echo=FALSE}
pred_test <- knn(Xtrain,Xtest,Ytrain,10)
sum(pred_test!=Ytest)/length(Ytest)
```

La valeur du taux d'erreur test est modifié car set.seed() fixe la graine de notre générateur aléatoire. En fixant la même graine on aura toujours les mêmes résultats pour n'importe quelle fonction R qui fait intervenir de l'aléatoire. La valeur "10" n'a pas de vrai importance, seulement qu'il occure pour le traitement immédiat, l'important c'est d'avoir toujours la même si nous voulons utiliser les mêmes données.

### Evaluer le taux d’erreur de la méthode knn (k = 10 voisins) en effectuant cette fois B = 100 découpages aléatoire des données. Représentez les taux d’erreurs test dans un boxplot et calculer le taux d’erreur test moyen.

Pour effectuer un découpage aléatoire on utilise la fonction R{base} sample qui permet la géneration aléatoire d'un échantilon (un sample) du jeu de donnée de base

```{r eval=FALSE}
B = 100
taux = rep(0, B) # on rempli notre taux de 0, comme on fait en Bayésiennes
```

On effectue donc une boucle et l'on calcul le taux d'erreur test sur chacun des découpages, on conserve les valeurs dans un vecteur.

```{r eval=FALSE}
B = 100
taux = rep(0, B) # on rempli notre taux de 0, comme on fait en Bayésiens

for(i in 1:B){   # on fait autant de fois la boucle qu'il y a de valeurs B
tr = sample(1:n, 800) # on reprend la question précédente
Xtrain = X[tr,] 
Ytrain = Y[tr] 
Xtest = X[-tr,] 
Ytest = Y[-tr] 

predi2 = knn(Xtrain, Xtest, Ytrain, 10)
taux_erreur = sum(predi2 != Ytest)/length(Ytest) 
taux[i] = taux_erreur  # on remplit notre taux qui était donc rempli de 0
}
```

On trace ensuite un boxplot pour avoir une idée du changement de valeur de nos taux d'erreur

```{r echo=FALSE}
B = 100
taux = rep(0, B) # on rempli notre taux de 0, comme on fait en Bayésiens

for(i in 1:B){   # on fait autant de fois la boucle qu'il y a de valeurs B
tr = sample(1:n, 800) # on reprend la question précédente
Xtrain = X[tr,] 
Ytrain = Y[tr] 
Xtest = X[-tr,] 
Ytest = Y[-tr] 

predi2 = knn(Xtrain, Xtest, Ytrain, 10)
taux_erreur = sum(predi2 != Ytest)/length(Ytest) 
taux[i] = taux_erreur  # on remplit notre taux qui était donc rempli de 0
}
boxplot(taux, xlab = "Erreur test", main = "B = 100 découpages")
```

On obtient un taux d'erreur test moyen aux alentours de 0.13. 13% est un taux d'erreur test raisonable, mais comment estimer la performance d'une règle de classification?

### Comparer ce taux d’erreur à celui de la règle de classification qui consisterait à toujours prédire la classe la plus probable à priori ?

Une méthode est de comparer le resultat de la règle de classification à une règle de classification élementaire, ici la plus simple règle de classification serait d'affecter à chacun la classe la plus probable (Ici 2), pour la définir il sufft d'observer un tableua de contingence des classes et de prendre la classe la plus grande. Graphiquement, on peut se réferrer aux diagramme en barre qui nous donnais les proportions de chacune des classes.

Notre tableau de contingence noous donne :
```{r echo=FALSE}
table(Y)/n
```
La règle du max à priori prédit toujours 2 au vu des proportions des classes (prédit la classe la plus probable à priori.
Nous obtenons un taux d'erreur de 0.196, contre 0.13 (lorsque donc avoir séparé les données en 80% apprentissage - 20% test). Ainsi, avoir séparé nos données a permis d'obtenir un taux d'erreur plus faible.

### On cherche maintenant à savoir si notre methode des knn prédit aussi bien les élements de la classe 1 et ceux de la classe 2

On introduit donc un nouvel outil que l'on nomme matrice de confusion définit comme suit:

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Matrice de Confusion & $Y_{Pred} = 1$  & $Y_{Pred} = 2$
\\
\hline
$Y_{Test} = 1$ & Vrais Positifs & Faux Négatifs \\
\hline
$Y_{Test} = 2$ & Faux Positifs & Vrais Négatifs \\
\hline
\end{tabular}
\end{center}

Cette matrice nous permet d'établir de nouvelles "mesures" d'éfficacité de notre modèle en précisant sur quelle classe notre modèle se "trompe" (effectue une mauvaise classification).
Tout d'abord, il est nécessaire d'affecter une classe dominante appelée 'positif'. Dans la matrice de confusion explécitée nous avons établi que 1 etait la classe des positifs. Avec ce postulat on a les taux d'erreur suivant :
$$\text{Taux de Vrais Positifs}=\frac{\text{Vrais Positifs}}{\text{Vrais Positifs + Faux Négatifs}}=\text{Sensibilité}$$
$$\text{Taux de Vrais Négatifs}=\frac{\text{Vrais Négatifs}}{\text{Vrai Négatifs + Faux Positifs}}=\text{Spécificité} $$
$$\text{Précision}=\frac{\text{Vrai Positif}}{\text{Vrai Positif + Faux Positifs}}$$
$$\text{F-Mesure}=2*\frac{\text{Sensibilité} * \text{Précision}}{\text{Sensibilité + Précision}}$$
On peut constater que la F-Mesure est une moyenne harmonique
On calcul donc ces diffèrentes "mesures":

```{r echo=FALSE}
pred_test <- knn(Xtrain, Xtest, Ytrain, 10)
tab <- table(Ytest, pred_test)
print("Matrice de Confusion")
tab
TVP = tab[1,1]/(tab[1,1]+tab[1,2])
print("Taux de Vrais Positifs ou Sensibilité")
TVP
TVN = tab[2,2]/(tab[2,2]+tab[2,1])
print("Taux de Vrais Négatifs ou Spécificité")
TVN
Pre = tab[1,1]/(tab[1,1]+tab[2,1])
print("Précision")
Pre
Sensi = TVP
Fmes = Pre*Sensi/(Pre+Sensi)
print("F-Mesure")
Fmes
taux_erreur = sum(pred_test != Ytest)/length(Ytest)
print("Taux d'erreur test")
taux_erreur
```

On observe que la classe la mieux prédite est la classe 2, $T_{VN} \approx 95\%$ contre $T_{VP} \approx 47\%$, ce qui est lié au fait que la classe 2 est très majoritaire dans le jeu de données

### Calcul de la F-Mesure pour positif = 'prédire 2'.

On change donc les calculs pour mettre "prédire 2" en temps que positifs et on obtient la comparaison des 2 F-Mesures suivantes :

```{r echo=FALSE}
Prec2 = tab[2,2]/(tab[2,2]+tab[1,2])
print(" Précision 'prédire 2' ")
Prec2
Sensi2 = TVN
print(" Sensibilité 'prédire 2' ")
Sensi2
Fmes2 = Prec2*Sensi2/(Prec2+Sensi2)
print(" F-Mesure pour positif = 'prédire 2' ")
Fmes2
print("F-Mesure pour positif = 'prédire 1' ")
Fmes
```

Tout d'abord on remarque que ici la Sensibilité revient au calcul du taux de vrais négatifs au dessus.
Et l'on remarque que la F-Mesure est bien supérieur lorsque l'on prend 'predire 2' comme positif, ce qui revient bien à notre conclusion précèdente : la classe 2 est mieux prédite que la classe 1.

## 2. On change maintenant pour se placer dans un cadre vraiment déséquilibrée, cependant, on conserve la matrice de coûts (0-1)

Le mélange gaussien sur lequel nous allons maintenant essayer d'établir une règle de classification et simulé à partir de la densité de lois suivante :

$$ 0.05 \times \mathbb{N}(\mu_1, \Sigma) + 0.95 \times \mathbb{N}(\mu_2, \Sigma)$$ 
On se retrouve donc dans un cadre vraiment très désequilibré.

```{r echo=FALSE}
library(mvtnorm)
n <- 1000
set.seed(10)
U =runif(n)
X <- matrix(NA,n,2)
Y <- rep(NA,n)
for(i in 1:n) {
  if(U[i] < .05) {
    X[i,] <- rmvnorm(1,c(0,0),diag(c(1,1)))
    Y[i] <- 1
  } else {
    X[i,] <- rmvnorm(1,c(2,0),diag(c(1,1)))
    Y[i] <- 2
  }
}
```

Graphiquement reproduit comme suit:

```{r}
plot(X, pch=Y, col=Y)
legend("topleft", legend=c("classe1", "classe2"), pch=1:2, col=1:2)
barplot(table(Y)/n, ylim = c(0,1), main = "Proportion des Classes")
```


### On effectue maintenant le découpage à 80/20

On utilise le même code que pour la question 1

```{r eval=FALSE}
set.seed(10)
tr <- sample(1:n,800)
Xtrain <- X[tr,]
Ytrain <- Y[tr]
Xtest <- X[-tr,]
Ytest <- Y[-tr]
```


### Evaluer le taux d’erreur de la méthode knn (k = 10 voisins) en effectuant cette fois B = 100 découpages aléatoire des données. Représentez les taux d’erreurs test dans un boxplot et calculer le taux d’erreur test moyen.

On éffectue la même procèdure que dans notre question 1 et on utilise toujours la fonction knn de R{class}, pour obtenir une erreur test de :

```{r echo=FALSE}
predi = knn(Xtrain, Xtest, Ytrain, 10)
taux_erreur = sum(predi != Ytest)/length(Ytest)
taux_erreur
```

Qui est quasiment égale à la règle de classification du max à priori (qui serait de 0.05), donc la procèdure est inutile dans ce cas là

### Calcul des TVP, TVN, F-Mesure avec positif = 'prédire 1'

On reprend là aussi les calculs de la question 1 est on obtient:

```{r echo=FALSE}
pred_test <- knn(Xtrain, Xtest, Ytrain, 10)
tab <- table(Ytest, pred_test)
print("Matrice de Confusion")
tab
TVP = tab[1,1]/(tab[1,1]+tab[1,2])
print("Taux de Vrais Positifs ou Sensibilité")
TVP
TVN = tab[2,2]/(tab[2,2]+tab[2,1])
print("Taux de Vrais Négatifs ou Spécificité")
TVN
Pre = tab[1,1]/(tab[1,1]+tab[2,1])
print("Précision")
Pre
Sensi = TVP
Fmes = Pre*Sensi/(Pre+Sensi)
print("F-Mesure")
Fmes
taux_erreur = sum(pred_test != Ytest)/length(Ytest)
print("Taux d'erreur test")
taux_erreur
```

On remarque un très faible taux de vrais positifs, un grand taux de vrais négatifs, une F-Mesure assez faible 

### On prend cette fois-ci positif = 'predire 2'

Les mêmes calculs en changeant le positif.
```{r echo=FALSE}
Prec2 = tab[2,2]/(tab[2,2]+tab[1,2])
print(" Précision 'prédire 2' ")
Prec2
Sensi2 = TVN
print(" Sensibilité 'prédire 2' ")
Sensi2
Fmes2 = Prec2*Sensi2/(Prec2+Sensi2)
print(" F-Mesure pour positif = 'prédire 2' ")
Fmes2
print("F-Mesure pour positif = 'prédire 1' ")
Fmes
```
Et on obtient cette fois ci une précision grande, et donc une F-Mesure bien meilleur, on a donc confirmation de notre intuition décrite sur le jeu de donnée faiblement disproportionné, lorsque le jeu de donnée est trop disproportionné, la classe majoritaire est bien mieux prédite que la classe minoritaire.
Mais qu'en est-il si l'erreur que nous effectuons en reclassant la plus petite classe dans la plus grande est plus grave que l'erreur de classer un élement de la grande classe dans la petite.
Un exemple de classification plus grave dans un sens que dans l'autre, est le cas du verdict dans un jugement, en effet il est considerer plus grave de condamner quelqu'un à tort que de ne pas condamner un coupable.

## 3. On reste dans le cadre désèquillibré, mais cette fois-ci on applique une matrice de coûts

Pour regler ce problème de cas de reclassification trop importante dans un sens que dans l'autre, on applique donc maintenant un matrice de coût $C$ comme suit :

```{r echo=FALSE}
C <- rbind(c(0,5), c(1,0))#Matrice de coûts
C
```

### Comment utiliser la méthode knn pour appliquer de manière empirique la règle de Bayes ? Quelle est la mesure de performance minimisée ?

On ne peut donc plus utiliser la fonction knn de R{class} qui n'est faite que pour traiter le cas d'une matrice de coût (O-1), il faut prendre en compte la matrice de coûts dans nos calculs de probabilité à posteriori $\mathbb{P}(Y=k|X=x)$, pour en obtenir la règle de classification de Bayes $g()$ qui minimise dans notre cas: 
$$g(x)=argmin\sum\limits_{k=1}^{2}C\mathbb{P}(Y=k|X=x)$$

Cela revient à affecter x a la classe la moins risqué à posteriori par un  risque pondéré par notre matrice de coût, où le calcul du risque est obtenue pour nos modalités de la variable $K$ par la formule ci-dessus. Qui nous donne pour le risque de classer x dans 1:

$$R(1|x)=C_{21}\mathbb{P}(Y=2|X=x)$$

, et le risque de classer x dans 2 par:

$$R(2|x)=C_{12}\mathbb{P}(Y=1|X=x)$$

Ici, on associe un coût $5$ fois supérieur aux faux positifs, ce qui nous donne comme coût moyen :
$$\widehat{\mathbb{E}}_{n}=\frac{5 \times  FP + FN}{n}$$
C'est un taux d'erreur pondéré. Sachant ceci est étant dans l'incapacité d'utiliser la fonction knn de R{class}, on se propose de coder une fonction pour inclure la possibilité d'interference d'une matrice de coût.

### Construction de la fonction knn_cout, et application aux données pour la prédiction.

```{r eval=FALSE}
knn_cout <- function(Xtrain, Ytrain, Xtest, C, k)
{
  #proba à posteriori d'être dans la classe 1 : P(Y=1/X=x)
  prob <- rep(NA, nrow(Xtest))
  res <- knn(Xtrain, Xtest, Ytrain, k=k, prob=TRUE)
  prob[res==1] <- attr(res,"prob")[res==1]
  prob[res==2] <- 1-attr(res,"prob")[res==2]
  #risque conditionnel de prédire la classe 1 : R(1/x)=C21*P(Y=2/X=x)
  R1 <- C[2,1]*(1-prob)
  #risque conditionnel de prédire la classe 2 : R(2/x)=C12*P(Y=1/X=x)
  R2 <- C[1,2]*prob
  #prediction avec la règle de Bayes et la matrice de coûts
  pred <- rep(NA, nrow(Xtest))
  pred[R1 < R2] <- 1
  pred[R2 < R1] <- 2
  return(pred)
}
pred_test <- knn_cout(Xtrain, Ytrain, Xtest, C, 10)
```

### Calcul des taux d'erreur risque empirique, TVP, TVN, et F-mes

Ici, on prend le statut positif='prédire 1', puisqu'il s'agit de la classe que l'on à pondérer 5 fois par rapport à la classe 2, et l'on trouve :

```{r echo=FALSE}
knn_cout <- function(Xtrain, Ytrain, Xtest, C, k)
{
  #proba à posteriori d'être dans la classe 1 : P(Y=1/X=x)
  prob <- rep(NA, nrow(Xtest))
  res <- knn(Xtrain, Xtest, Ytrain, k=k, prob=TRUE)
  prob[res==1] <- attr(res,"prob")[res==1]
  prob[res==2] <- 1-attr(res,"prob")[res==2]
  #risque conditionnel de prédire la classe 1 : R(1/x)=C21*P(Y=2/X=x)
  R1 <- C[2,1]*(1-prob)
  #risque conditionnel de prédire la classe 2 : R(2/x)=C12*P(Y=1/X=x)
  R2 <- C[1,2]*prob
  #prediction avec la règle de Bayes et la matrice de coûts
  pred <- rep(NA, nrow(Xtest))
  pred[R1 < R2] <- 1
  pred[R2 < R1] <- 2
  return(pred)
}
pred_test <- knn_cout(Xtrain, Ytrain, Xtest, C, 10)
tab <- table(Ytest, pred_test)
print("Matrice de Confusion")
tab
TVP = tab[1,1]/(tab[1,1]+tab[1,2])
print("Taux de Vrais Positifs ou Sensibilité")
TVP
TVN = tab[2,2]/(tab[2,2]+tab[2,1])
print("Taux de Vrais Négatifs ou Spécificité")
TVN
Pre = tab[1,1]/(tab[1,1]+tab[2,1])
print("Précision")
Pre
Sensi = TVP
Fmes = Pre*Sensi/(Pre+Sensi)
print("F-Mesure")
Fmes
taux_erreur = sum(pred_test != Ytest)/length(Ytest)
print("Taux d'erreur test")
taux_erreur
```

On obtient donc de bien meilleur résultat que lorsqu'on utilise la méthode des knn sans matrice $C$, prenons par exemple la comparaison des F-Mesures, on a sans matrice de coût $C$, $F_{Mes} \approx 0.3$, et F-Mesure avec la matrice de coût $C$, $F_{Mes} \approx 0.4$, mais là où la diffèrence est encore plus importante, c'est dans le taux de vrais positifs, en effet toujours, dans le cas positif = 'predire 1', on a sans matrice $C$, $T_{VP} \approx 0.5$, avec $C$, $T_{VP} \approx 0.8$. 
Existe-t-il des méthodes pour rendre ces methodes de classifications encore plus robuste?


### Calcul de la F-mesure par validation croisée 5-Folds

La méthode de validation croisée 5-Folds, est une méthode de subdivision de notre jeu de donnée en $k$-$folds$, c'est à dire $k$-sous ensemble de même taille de notre jeu de donnée en répartissant comme $1$ sous-ensemble qui sert de données test, et les $4$ autres qui servent de données d'apprentissage pour construire notre modèle, il y a donc $5$ procédure de modélisation sur les données

```{r echo=FALSE}
knn_cout <- function(Xtrain, Ytrain, Xtest, C, k)
{
  #proba à posteriori d'être dans la classe 1 : P(Y=1/X=x)
  prob <- rep(NA, nrow(Xtest))
  res <- knn(Xtrain, Xtest, Ytrain, k=k, prob=TRUE)
  prob[res==1] <- attr(res,"prob")[res==1]
  prob[res==2] <- 1-attr(res,"prob")[res==2]
  #risque conditionnel de prédire la classe 1 : R(1/x)=C21*P(Y=2/X=x)
  R1 <- C[2,1]*(1-prob)
  #risque conditionnel de prédire la classe 2 : R(2/x)=C12*P(Y=1/X=x)
  R2 <- C[1,2]*prob
  #prediction avec la règle de Bayes et la matrice de coûts
  pred <- rep(NA, nrow(Xtest))
  pred[R1 < R2] <- 1
  pred[R2 < R1] <- 2
  return(pred)
}
Fmesure <- function(Y,pred)
{
  tab <- table(Y,pred)
  tvp <- tab[1,1]/(tab[1,1]+tab[1,2])
  precision <- tab[1,1]/(tab[1,1]+tab[2,1])
  2*precision*tvp/(precision+tvp)
}
set.seed(10)
n_folds <- 5
folds_i <- sample(rep(1:n_folds, length.out = n))
pred <- rep(NA, nrow(X))
for (k in 1:n_folds) {
  test_i <- which(folds_i == k)
  Xtrain <- X[-test_i, ]
  Xtest <- X[test_i, ]
  pred[test_i] <- knn_cout(Xtrain, Y[-test_i], Xtest, C, k=10)
}
# F-mesure calculé par validation croisée 5-folds
print("F-Mesure 5-Folds")
Fmesure(Y, pred)
```


### On retire maintenant la commande set.seed()

Comme précèdement dans le calcul du taux d'erreur test, on se propose là encore de retirer la commande set.seed() de R{base}, et on obtient les résultats suivants:

```{r echo=FALSE}
n_folds <- 5
folds_i <- sample(rep(1:n_folds, length.out = n))
pred <- rep(NA, nrow(X))
for (k in 1:n_folds) {
  test_i <- which(folds_i == k)
  Xtrain <- X[-test_i, ]
  Xtest <- X[test_i, ]
  pred[test_i] <- knn_cout(Xtrain, Y[-test_i], Xtest, C, k=10)
}
print("F-Mesure 5-Folds sans set.seed()")
Fmesure(Y, pred)
```

On remarque que lorsque la commande est retiré, la F-Mesure calculé change à chaque nouvel application de la commande, c'est comme observé plus haut à cause du fait que lorsque l'on ne fixe pas de graines les tirages aléatoires sont réexecuter à chaque fois.


### On effectue maintenant 20 découpages aléatoires 5-Folds pour observer l'étalement dans un boxplot de la F-mesure

La validation croisée $5$-$folds$, est une très bonne méthode pour rendre plus robuste notre règle de classification, cependant, il existe encore une dépendance de notre modèle à la manière dont les sous-ensembles ont étaient créés, pour limiter l'impact de ce découpages en $5$, une méthode est d'effectuer plusieurs fois ces $5$ découpages.
Dans le code ci-dessous nous avons effectuer $20$ itérations de découpage aléatoires en $5$. Le résultat est un boxplot des $20$ F-Mesures obtenues par validation croisée $5$-$folds$ : 

```{r echo=FALSE}
B = 20
VFMe = rep(0, B) # on rempli notre taux de 0, comme on fait en Bayésiennes

for(i in 1:B){   # on fait autant de fois la boucle qu'il y a de valeurs B
  n_folds <- 5
  folds_i <- sample(rep(1:n_folds, length.out = n))
  pred <- rep(NA, nrow(X))
  for (k in 1:n_folds) {
    test_i <- which(folds_i == k)
    Xtrain <- X[-test_i, ]
    Xtest <- X[test_i, ]
    pred[test_i] <- knn_cout(Xtrain, Y[-test_i], Xtest, C, k=10)
  
    }
  VFMe[i]<-Fmesure(Y, pred)
}
boxplot(VFMe, xlab = "F-Mesures", main = "B = 100 découpages")
print("F-Mesure Moyenne :")
mean(VFMe)
```

On obtient également une F-Mesure moyenne.

\newpage

# Exercice 3

## 1. Simulation d'un mélange de gaussiennes
Pour ce nouvel exercice, nous nous proposons de travailler sur un jeu de donnée, que nous aurions simulé au préalable. Pour se faire on utilise un mélange de loi gaussienne obtenue avec la fonction R{mvtnorm}, qui s'appelle rmvnorm()
Le mélange gaussien satisfait l'équation suivante : 

$$0.5 \times \mathbb{N}(\mu_1, \Sigma) + 0.5 \times \mathbb{N}(\mu_2, \Sigma)$$ 
On utilise donc le code pour la simulation,
```{r echo=FALSE}
library(mvtnorm)
n <- 1000
set.seed(10)
U =runif(n)
X <- matrix(NA,n,2)
Y <- rep(NA,n)
for(i in 1:n) {
  if(U[i] < .5) {
    X[i,] <- rmvnorm(1,c(0,0),diag(c(1,1)))
    Y[i] <- 1
  } else {
    X[i,] <- rmvnorm(1,c(2,0),diag(c(1,1)))
    Y[i] <- 2
  }
}
```

Ce qui nous donne lorsqu'on le projette graphiquement :

```{r echo=FALSE}
plot(X, pch=Y, col=Y)
legend("topleft", legend=c("classe1", "classe2"), pch=1:2, col=1:2)
barplot(table(Y)/n, ylim = c(0,1), main = "Proportion des Classes")
```

## 2. Découpage de l'échantillon en Apprentissage/Test.

On découpe notre échantillon simulé en deux parties :
\begin{itemize}
  \item{Une partie qui va être la base de notre modèle.}
  \item{Une partie qui nous servira a évaluer notre méthode de classification sur un jeu de donnée qui n'aura pas servi à la création du modèle.}
\end{itemize}
Ce genre de pratique est presque toujours utilisé, elle permet de s'assurer que le modèle ne correspond pas uniquement à une tendance uniquement lié au jeu de donnée qui sert de construction au modèle.
```{r eval=FALSE}
set.seed(10)
tr <- sample(1:n,800)
Xtrain <- X[tr,]
Ytrain <- Y[tr]
Xtest <- X[-tr,]
Ytest <- Y[-tr]
```

## 3. Choix du k qui minimise l'erreur de validation

Par ce découpage on peut calculer ce que l'on appelle l'erreur de validation. C'est à dire que vu que l'on connait la vrai classe de l'ensemble des jeux de données d'apprentissage et de validation, on se propose de prédire la classe des observations de validation, et donc de comparer les classes prédites par le modèle avec les vraies classes. Cela fera intervenir les notions de vrais, ou faux, positifs, ainsi que, de vrais ou de faux négatifs, dont nous discuterons plus tard.
Une méthode pour la sélection d'un nombre k de voisins optimal et de choisir le k qui minimise ette erreur de validation.

### a) Découpage de l'échantillon apprentissage en deux échantillons apprentissage/validation

On réutilise les méthodes classiques de découpages avec la fonction sample() de R{base}, en incluant le set.seed() dans notre construction.

```{r eval=FALSE}
set.seed(10)
tr <- sample(1:800,640)
Xtrain_i <- Xtrain[tr,]
Ytrain_i <- Ytrain[tr]
Xvalid <- Xtrain[-tr,]
Yvalid <- Ytrain[-tr]
```

### b) Calcul du taux d'erreur de validation pour k variant

On se choisit de prendre k variant de 1 à 30, pour le calcul du taux d'erreur de validation. Le k minimisant l'erreur de validation est le k choisit optimal pour l'utilisation du modèle. 

```{r eval=FALSE}
kmax <- 30
err <- rep(NA,kmax)
for (k in 1:kmax){
  pred <- knn(Xtrain_i,Xvalid,Ytrain_i,k)
  err[k] <- sum(pred!=Yvalid)/length(Yvalid)
}
```

Pour observer graphiquement l'évolution de ce taux d'erreur on le trace en fonction des $k$. On obtient :

```{r echo=FALSE}
tr <- sample(1:800,640)
Xtrain_i <- Xtrain[tr,]
Ytrain_i <- Ytrain[tr]
Xvalid <- Xtrain[-tr,]
Yvalid <- Ytrain[-tr]
kmax <- 30
err <- rep(NA,kmax)
for (k in 1:kmax){
  pred <- knn(Xtrain_i,Xvalid,Ytrain_i,k)
  err[k] <- sum(pred!=Yvalid)/length(Yvalid)
}
plot(err,type="l",xlab="Nombre de voisins", ylab="erreur de validation", ylim=c(0,max(err)))
```

### c) On recommence avec un autre découpage 

```{r echo=FALSE}
tr <- sample(1:800,640)
Xtrain_i <- Xtrain[tr,]
Ytrain_i <- Ytrain[tr]
Xvalid <- Xtrain[-tr,]
Yvalid <- Ytrain[-tr]
kmax <- 30
err <- rep(NA,kmax)
for (k in 1:kmax){
  pred <- knn(Xtrain_i,Xvalid,Ytrain_i,k)
  err[k] <- sum(pred!=Yvalid)/length(Yvalid)
}
plot(err,type="l",xlab="Nombre de voisins", ylab="erreur de validation", main="Autre découpage", ylim=c(0,max(err)))
```

On se rend compte que le k optimal dépend du découpages, on va donc effectuer une boucle pour avoir sur plusieurs découpages, le nombre de voisins $k$ optimal moyen.

```{r echo=FALSE}
B <- 20 
error <- matrix(NA, nrow = kmax, ncol = B)
kmax <- 30
for (b in 1:B) {
  tr <- sample(1:800, 640)
  Xtrain_i <- X[tr,]
  Xvalid <- X[-tr,]
  Ytrain_i <- Y[tr]
  Yvalid <- Y[-tr]
  for (k in 1:kmax){
    pred <- knn(Xtrain_i, Xvalid, Ytrain_i, k=k)
    error[k,b] <- sum(Yvalid != pred)/length(Yvalid)
  }
}

matplot(error, type = "l", col = 'red', ylim = c(0, max(error)), main = 'B=20 decoupages apprentissage validation')

mean_error_valid <- apply(error, 1, mean) #on regarde l'erreur moyenne à k fixé p
matpoints(mean_error_valid, type = 'l', col ="red", lwd = 4)

legend('bottomright', legend = c("Erreur moyenne", "Erreurs conditionnelles"), lty = c(1,3), lwd = c(4,1), col = c(2,2))
k_opt <- which.min(mean_error_valid)
print("Nombre de voisins optimal avec 20 découpages")
k_opt
```


### d) Prédictions avec le k-optimal obtenue plus haut

On se sert donc du $k$-optimal obtenue plus haut, on obtient donc les predictions pour notre jeux de données test, et l'erreur test associè :

```{r echo=FALSE}
pred_test <- knn(Xtrain, Xtest, Ytrain, k_opt)
taux_erreur = sum(pred_test != Ytest)/length(Ytest)
print("Taux d'erreur test pour k-optimal")
taux_erreur
```



## 4. Predire à l'aide de la fonction knn.cv : méthode LOO

Une autre manière de calculer l'erreur de validation lorsque l'on a un jeu de donnée déjà découpé apprentissage|test, c'est à dire avant de recouper le jeu apprentissage en apprentissage|test, s'appelle la méthode Leave One Out (L.O.O.). Celle-ci consiste a récupèrer tout le jeu de donnée apprentissage et de ne laisser qu'un seul élèment du jeu de donnée en validation et effectuer cela pour tout le jeu de donnée.

### Erreur de validation avec knn.cv pour k=10

Il existe dans R{class}, une fonction knn.cv() (pour knn.cross-validation), qui effectue directement ce traitement, on utilise donc cette fonction pour le calcul de l'erreur de validation :

```{r echo=FALSE}
pred <- knn.cv(Xtrain, Ytrain, k=10)
print("Erreur de validation par méthode LOO")
sum(pred!=Ytrain)/length(Ytrain)
```

### Calcul pour k variants

On utilise maintenant la même méthode pour k variant de 1 à 30, et on calcul l'erreur de validation croisée pour chaque $k$

```{r echo=FALSE}
pred = knn.cv(Xtrain, Ytrain, k=1)
err = sum(pred!=Ytrain)/length(Ytrain)
for (k in 2:30){
  pred = knn.cv(Xtrain, Ytrain, k)
  err <- cbind(err, sum(pred!=Ytrain)/length(Ytrain))
}
plot(as.vector(err),type="l",xlab="Nombre de voisins", ylab="erreur de validation", ylim=c(0,max(err)))

```

On obtient donc pour $k$-optimal : 

```{r echo=FALSE}
k_opt <- which.min(err)
k_opt
```

### Calcul du taux d'erreur test avec le k-optimal :

```{r echo=FALSE}
pred = knn(Xtrain, Xtest, Ytrain, k=k_opt)
err_test = sum(pred!=Ytest)/length(Ytest)
print("erreur test avec k optimal méthode LOO")
err_test
```

### Critique de l'approche & Cadre plus spécifique optimal pour cette méthode :

Cette méthode n'est pas optimal dans notre cas car nous possédons déjà assez de données pour le découpage en apprentissage|validation|test, la méthode LOO, dans ce cas là ira bien trop profondement dans les données et risquerait de nous sur-apprendre.
Un cadre optimal pour la validation LOO est le cas où le jeu de donnée est vraiment trop petit, dans ce cas là même avec une taille d'échanillon $n=30$ on arrive à construire un modèle assez robuste avec cette méthode 

## 5. Proposez une autre méthode pour le choix du k-optimal nombre de voisins

Une idée serait la méthodes des k-folds pour la sélection du k optimal, prenons ici une méthode $5$-$folds$

## 6. Appliquons B=15 découpages du jeu apprentissage|test pour la méthode LOO

Comme pour la validation croisée $k$-$folds$, On va effectuer un boucle sur les $15$ diffèrents découpages apprentissage|test, pour en obtenir un boxplot et une erreur test moyenne :

```{r echo=FALSE}
B <- 15
kmax <- 30
err_test <- rep(NA, B)
for (b in 1:B) {
  tr <- sample(1:1000, 800)
  Xtrain <- X[tr,]
  Xtest <- X[-tr,]
  Ytrain <- Y[tr]
  Ytest <- Y[-tr]
  pred = knn.cv(Xtrain, Ytrain, k=1)
  err = sum(pred!=Ytrain)/length(Ytrain)
  for (k in 2:kmax){
    pred = knn.cv(Xtrain, Ytrain, k)
    err <- cbind(err, sum(pred!=Ytrain)/length(Ytrain))
    k_opt <- which.min(err)
    }
  pred_test <- knn(Xtrain, Xtest, Ytrain, k_opt)
  err_test[b] <- sum(pred_test!=Ytest)/length(Ytest)
}
boxplot(err_test, main = 'B=15 decoupages apprentissage|test méthode LOO', ylab="Erreur Test")
```


\newpage

# Exercice 4

## 1.Chargement des données

On charge tout d'abord les données,

```{r}
load("Desbois.rda")
```

et on vérifie leur contenance :

```{r echo=FALSE}
print("structure des données")
dim(data)
print("1260 obsérvations de 5 variables")
X <- data[,-1] # On doit verifier qu'il s'agit bien de variable quantitative
Y <- data$DIFF
print("Exploitations saines | défaillantes")
table(Y)# Pour verifier les exploitations suivants leurs type (saines ou defaillantes)
```



## 2.Evaluation du nombre de voisins optimal pour la règle de décision

On éffectue une validation croisée sur $20$ découpages pour un $k$ variant de $1$ à $60$, et l'on observe les erreurs de validations moyennes pour obtenir le $k$ nombre de voisins qui minimise cette erreur

```{r echo=FALSE}
B <- 20
kmax <- 60
err_valid <- matrix(NA, kmax,B)
for (b in 1:B){
    tr <- sample(1:nrow(X),900)
    Xtrain <- X[tr,]
    Ytrain <- Y[tr]
    Xtest <- X[-tr,]
    Ytest <- Y[-tr]
    Ytest <- Y[-tr]
  for (k in 1:kmax){
    pred <- knn(Xtrain, Xtest, Ytrain, k)
    err_valid[k,b] <- sum(pred!=Ytest)/length(Ytest)
  }
}
matplot(err_valid,type="l",lty=2,col=2, ylim = c(0,0.2),
        xlab="nombre de voisins",ylab="taux d'erreur de validation",
        main="B=20 découpages apprentissage/validation, ")
mean_err_valid <- apply(err_valid,1,mean)
matpoints(mean_err_valid,type="l",col=2,lwd=4)
legend("bottomright", lty=c(1,3),lwd=c(4,2),col=c(2,2),
       legend=c("Erreur moyenne", "Erreurs conditionnelles"))

```


### En quoi ce graphique peut être rassurant avant d'appliquer une procédure de choix automatique de k ?

Tout d'abord en raison du fait que le taux d'erreur moyen se situe autour de 13%, avec cette methode, et de plus par le fait qui se stabilise assez rapidement, autour d'un nombre de voisin choisi à $k=8$, donc assez un nombre assez petit de voisin 


## 3. Seconde procédure automatique de sélection du k-opt

On effectue donc une méthode de sélection du $k$ optimal par minimisation de l'erreur de validation croisée avec la méthode LOO, en prenant en compte les calculs de taux de bon reclassement, $T_{bc}$, $T_{VP}$, et de $T_{VN}$
Le taux de bons reclassements n'a jamais était explicité, mais comme vous pouvez vous en douter, il est donné par :
$$\text{Taux de bon reclassement}=\frac{\text{T}_{VP} + \text{T}_{VN}}{\text{Longueur de l'échantillon}}$$

### (a) Premier découpage

On effectue un premier découpage aléatoire pour la sélection automatique, et on obtient

```{r echo=FALSE}
kmax <- 40
err_valid <- rep(NA, kmax)
set.seed(10)
tr <- sample(1:nrow(X), 945)
Xtrain <- X[tr,]
Ytrain <- Y[tr]
Xtest <- X[-tr,]
Ytest <- Y[-tr]
for (k in 1:kmax){
  pred <- knn.cv(Xtrain, Ytrain, k)
  err_valid[k] <- sum(pred!=Ytrain)/length(Ytrain)
}
kopt <- which.min(err_valid) # Ici le minimiseur de l'erreur est obtenue pour k=13
pred <- knn(Xtrain, Xtest, Ytrain, k=kopt) # On effectue les predictions avec notre methode de classification pour notre k-opt
C <- table(Ytest,pred) # Et on obtient la matrice de confusion pour le calculs des diffÃ©rents taux de mesure de la classification 
tbc_test <- (C[1,1]+ C[2,2])/length(Ytest) # Le taux de bon classement obtenue est de 92.1%
tvp_test <- C[2,2]/(C[2,1]+C[2,2]) # Le taux de vrai positif avec prÃ©dire 1 := positif est de 90.4%
tvn_test <- C[1,1]/(C[1,1]+C[1,2]) # En conservant la dÃ©finition ci-dessus le taux de vrai négatif est de 0.93.5%
print("K optimal")
kopt
print("Taux de bons reclassement")
tbc_test
print("Taux de vrais positifs")
tvp_test
print("Taux de vrais négatifs")
tvn_test
```

Le bon déroulement de cette routine, nous permet de l'affecter dans une boucle qui parcourera plusieurs découpages du jeux de données en Apprentissage|Test, et nous permettra d'obtenir des boxplot des diffèrentes valeurs de $T_{bc}$, $T_{VP}$, et de $T_{VN}$

### (b) Avec les 20 découpages

```{r echo=FALSE}
B = 20
kmax=30
for (i in 1:B-1){
  tr <- sample(1:nrow(X), 945)
  Xtrain <- X[tr,]
  Ytrain <- Y[tr]
  Xtest <- X[-tr,]
  Ytest <- Y[-tr]
  for (k in 1:kmax){
    pred <- knn.cv(Xtrain, Ytrain, k)
    err_valid[k] <- sum(pred!=Ytrain)/length(Ytrain)
  }
  kopt <- which.min(err_valid)
  pred <- knn(Xtrain, Xtest, Ytrain, k=kopt)
  C <- table(Ytest,pred)
  tbc_test <- rbind(tbc_test, (C[1,1]+ C[2,2])/length(Ytest))
  tvp_test <- rbind(tvp_test,C[2,2]/(C[2,1]+C[2,2]))
  tvn_test <- rbind(tvn_test,C[1,1]/(C[1,1]+C[1,2]))
}
boxplot(cbind(tbc_test, tvp_test, tvn_test), names = c("Taux Bons Reclass", "Taux Vrais Posi", "Taux Vrais Nég"), main = "B = 20 découpages App|Test")
```

Ce premier graphique, nous donne les differents taux de mesure de validation de notre règle de classification pour les 20 découpages, on observe des taux moyen assez élevés aux alentours de 90% pour toutes les mesures (tbc, tvp, tvn), on peut observer une variance un peu  plus grande pour le tvn, même si cela reste assez léger.

```{r echo=FALSE}
boxplot(cbind(tbc_test, tvp_test, tvn_test), names = c("Taux Bons Reclass", "Taux Vrais Posi", "Taux Vrais Nég"), main = "B = 20 découpages App|Test", ylim=c(0,1))
```

Le second graphique, représente les mêmes mesures seulement on a agrandi l'échelle de representation, en effet, ici le graphe  démarre de l'origine (0,0) ce qui nous permet de relativiser les vrais diffèrences de variances expliquées plus haut et de se dire que c'est taux sont relativement similaires.

## 4. Méthodes des k-nn validées pour la prédiction d'une nouvelle mesure

La volonté première lorsque l'on éffectuer des règles de classification reste tout d'abord comme dans de nombreuses méthodes statistiques, la volonté de prédire la classe d'une nouvelle observation $x$, ici savoir lorsque l'on rentre les données d'une nouvelle exploitation agricole, si celle-ci aura une santé fonancière saine ou défaillante, pour se faire après les découpages pour obtenir, le $k$ nombre de voisins optimal, validé par l'ensemble test, on se sert de l'ensemble du jeu de donées pour établir notre modèle puis on applique ce modèle au données à classifier.

```{r echo=FALSE}
Xhat = c(0.700,0.300,0.100,0.500)
```

```{r eval=FALSE}
Yhat = knn(X, Xhat, Y, k=kopt)
```

Et on obtient la classification : 

```{r echo=FALSE}
Yhat = knn(X, Xhat, Y, k=kopt)
Yhat
```

Cette nouvelle observation sera donc classer comme une exploitation défaillante.

\newpage

# Exercice 5

Pour la classification binaire, il existe un autre méthode pour l'boutissement d'une règle, basé sur ce que l'on appelle un score et une courbe $ROC$

## Chargement des données

```{r eval=FALSE}
load("Desbois.rda")
X <- data[,-1]
Y <- data$DIFF
```


## 1. Découpage aléatoire

Toujours avec la même méthode avec les fonctions set.seed(), et sample(), de R{base}

```{r}
set.seed(10)
tr <- sample(1:nrow(X),945)
Xtrain <- X[tr,]
Ytrain <- Y[tr]
Xtest <- X[-tr,]
Ytest <- Y[-tr]
```

## 2. Calcul du score avec la fonction k-nn

On s'interesse ici à un calcul de score, pour la création d'un tel objet il faut tout d'abord le définir et établir sont domaine d'exercice, il faut tout d'abord, comme dans les calculs de $T_{VP}$, définir une classe "positif", le score correspond alors à la probabilité que l'élement $x$ appartienne à la classe positive. On se sert donc d'un calcul de probabilité à priori.
$$p=\mathbb{P}(Y=1|X=x)$$

On calcul cette probabilité à l'aide de la fonction knn de R{class}, avec le code ci-dessous :

```{r eval=FALSE}
prob_knn <- function(Xtrain, Xtest, Ytrain, k){
  #proba à posteriori d'être dans la classe 1 : P(Y=1/X=x)
  prob <- rep(NA, nrow(Xtest))
  res <- knn(Xtrain, Xtest, Ytrain, k=k, prob=TRUE)
  prob[res==1] <- attr(res,"prob")[res==1]
  prob[res==0] <- 1-attr(res,"prob")[res==0]
  return(prob)
}
```

Et on obtient comme score pour les premières données du jeu de donnée test, avec ($k=30$) voisins :

```{r echo=FALSE}
prob_knn <- function(Xtrain, Xtest, Ytrain, k){
  #proba à posteriori d'être dans la classe 1 : P(Y=1/X=x)
  prob <- rep(NA, nrow(Xtest))
  res <- knn(Xtrain, Xtest, Ytrain, k=k, prob=TRUE)
  prob[res==1] <- attr(res,"prob")[res==1]
  prob[res==0] <- 1-attr(res,"prob")[res==0]
  return(prob)
}
score <- prob_knn(Xtrain, Xtest, Ytrain, k = 30)
head(score)
```



## 3. Courbe ROC pour le cas k=30 voisins

Exemple de courbe $ROC$ (\textit{Receiver Operating Characteristic})

```{r echo=FALSE}
library(ROCR) # 3 fonctions : prediction, performance, plot
pred <- prediction(score, Ytest, label.ordering=c("0","1"))
#label.ordering : indiquer le libellÃ© de la classe negative puis positive.
perf <- performance(pred, "tpr", "fpr")
plot(perf) #courbe ROC
abline(a=0, b=1)
```

La courbe $ROC$, est déssiné en traçant le taux de vrais positifs par rapport au taux de faux positifs, il faut voir le point d'origine comme la règle de base où l'on classe tout les éléments en négatifs, et donc en $(1,1)$, on reclasse tout en positif. On compare souvent cette courbe à la doite $y=x$, qui serait la règle de classification du max a priori, pour toute les nouvelles données. Plus la courbe s'éloigne de cette droite plus la règle est bonne. Pour calculer cette éfficacité de classification, on calcul l'aire sous la courbe  $ROC$

### Calcul de l'aire sous la courbe ROC en mesure du qualitée du score

```{r echo=FALSE}
auc <- performance(pred, "auc")@y.values[[1]]
print("Score Obtenue par aire sous la courbe ROC")
auc
```

On obtient donc un score trés proche de 1, qui serait la règle de classification parfaite, on a donc un trés bon score avec cette règle, on observe génèralement cela comme décrit plus haut par une courbe qui s'écarte bien de la diagonale entre $(0,0)$ & $(1,1)$.

## 4. Calcul du critère AUC pour 50 découpages

```{r echo=FALSE}
B=50
auc_vect <- rep(NA, B)
prob_knn <- function(Xtrain, Xtest, Ytrain, k){
  #proba à posteriori d'être dans la classe 1 : P(Y=1/X=x)
  prob <- rep(NA, nrow(Xtest))
  res <- knn(Xtrain, Xtest, Ytrain, k=k, prob=TRUE)
  prob[res==1] <- attr(res,"prob")[res==1]
  prob[res==0] <- 1-attr(res,"prob")[res==0]
  return(prob)
}
for (i in 1:B){
  tr <- sample(1:nrow(X),945)
  Xtrain_i <- X[tr,]
  Ytrain_i <- Y[tr]
  Xtest_i <- X[-tr,]
  Ytest_i <- Y[-tr]
  score <- prob_knn(Xtrain_i, Xtest_i, Ytrain_i, k = 30)
  pred <- prediction(score, Ytest_i, label.ordering=c("0","1"))
  auc_vect[i] <- performance(pred, "auc")@y.values[[1]]
}
boxplot(auc_vect, ylab="AUC aire sous la courbe ROC", main="AUC pour 50 découpages Apprentissage|Test")
```


## 5. Tracer de la courbe ROC sans le package

On se sert maintenant des $T_{VP}$ et des $T_{FP}$, pour le tracer de la courbe $ROC$, en fonciton du seuil, $s$.

```{r eval=FALSE}
ClassSC <- function(score, seuil){
  Y_test_SC<-rep(NA,length(score))
  for (i in 1:length(score)){
      if (score[i] < seuil){
      Y_test_SC[i] = 0
    }else {Y_test_SC[i] = 1}
  }
  Y_test_SC
}

TVP <- function(Ypred, Yvrai){
  TPR=0
  for (i in 1:length(Ypred)){
    if (Ypred[i]==1 | Yvrai[i]==1) 
    TPR <- TPR+1
  }
  TPR <- TPR/length(Ypred)
  TPR
}

TFP <- function(Ypred, Yvrai){
  FPR=0
  for (i in 1:length(Ypred)){
    if (Ypred[i]==1 | Yvrai[i]==0) 
    FPR <- FPR+1
  }
  FPR <- FPR/length(Ypred)
  FPR
}

CourbeROC <- function(score, Ytest){
  seuil <- seq(0,1,0.01)
  for (i in 1:length(seuil)){
  Y_test <- ClassSC(score, seuil[i])
  TPR[i] <- TVP(Y_test, Ytest)
  FPR[i] <- TFP(Y_test, Ytest)
  }
}
```

Et l'on test la fonction

```{r echo=FALSE}
ClassSC <- function(score, seuil){
  Y_test_SC<-rep(NA,length(score))
  for (i in 1:length(score)){
      if (score[i] < seuil){
      Y_test_SC[i] = 0
    }else {Y_test_SC[i] = 1}
  }
  Y_test_SC
}

TVP <- function(Ypred, Yvrai){
  TPR=0
  for (i in 1:length(Ypred)){
    if (Ypred[i]==1 | Yvrai[i]==1) 
    TPR <- TPR+1
  }
  TPR <- TPR/length(Ypred)
  TPR
}

TFP <- function(Ypred, Yvrai){
  FPR=0
  for (i in 1:length(Ypred)){
    if (Ypred[i]==1 | Yvrai[i]==0) 
    FPR <- FPR+1
  }
  FPR <- FPR/length(Ypred)
  FPR
}

CourbeROC <- function(score, Ytest){
  seuil <- seq(0,1.1,0.01)
  for (i in 1:length(seuil)){
  Y_test <- ClassSC(score, seuil[i])
  TPR[i] <- TVP(Y_test, Ytest)
  FPR[i] <- TFP(Y_test, Ytest)
  }
}

```

Il semble y avoir un problème dans ma fonction sûrement une incompréhension de ce qu'est vraiment la courbe $ROC$

## 6. Tracer de la courbe ROC pour 20 découpages

```{r echo=FALSE, warning=FALSE}
B=19
plot(perf, main="Courbe ROC pour 20 découpages")
FPR <- as.vector(perf@x.values)[[1]]
TPR <- as.vector(perf@y.values)[[1]]
perf_mat <- perf
prob_knn <- function(Xtrain, Xtest, Ytrain, k){
  #proba à posteriori d'être dans la classe 1 : P(Y=1/X=x)
  prob <- rep(NA, nrow(Xtest))
  res <- knn(Xtrain, Xtest, Ytrain, k=k, prob=TRUE)
  prob[res==1] <- attr(res,"prob")[res==1]
  prob[res==0] <- 1-attr(res,"prob")[res==0]
  return(prob)
}
for (i in 1:B){
  tr <- sample(1:nrow(X),945)
  Xtrain_i <- X[tr,]
  Ytrain_i <- Y[tr]
  Xtest_i <- X[-tr,]
  Ytest_i <- Y[-tr]
  score <- prob_knn(Xtrain_i, Xtest_i, Ytrain_i, k = 30)
  pred <- prediction(score, Ytest_i, label.ordering=c("0","1"))
  perf <- performance(pred, "tpr", "fpr")
  par(new=TRUE)
  plot(perf)
  perf_mat <- cbind(perf,perf_mat)
  FPR <- cbind(FPR, as.vector(perf@x.values)[[1]])
  TPR <- cbind(TPR, as.vector(perf@y.values)[[1]])
}
```


## 7. Utilisation du score comme règle de classification.

Dans cette question on cherche à obtenir le seuil pour un découpage Apprentissage|Test qui maximise la F-Mesure.

```{r echo=FALSE}
pred <- prediction(score, Ytest, label.ordering=c("0","1"))
#label.ordering : indiquer le libellÃ© de la classe negative puis positive.
FMES <- performance(pred, "f")
plot(FMES)
```

On a donc que le seuil, $s$, qui maximise la F-Mesure, est obtenue pour :

```{r echo=FALSE}
print("Seuil maximisant la F-Mesure")
performance(pred, "f")@x.values[[1]][which.max(performance(pred, "f")@y.values[[1]])]
```


## 8. Calcul pour 20 découpages Apprentissage|Test

On effectue ce même calcul de F-Mesure pour 20 découpages, 

```{r echo=FALSE}
B=20
FMES_vect = rep(NA, B)
for (i in 1:B){
  tr <- sample(1:nrow(X),945)
  Xtrain_i <- X[tr,]
  Ytrain_i <- Y[tr]
  Xtest_i <- X[-tr,]
  Ytest_i <- Y[-tr]
  score <- prob_knn(Xtrain_i, Xtest_i, Ytrain_i, k = 30)
  pred <- prediction(score, Ytest_i, label.ordering=c("0","1"))
  FMES_vect[i] <- performance(pred, "f")@x.values[[1]][which.max(performance(pred, "f")@y.values[[1]])]
}
boxplot(FMES_vect, xlab="Seuil Optimal", ylab="F-Mesure test", main="F-Mesure pour 20 découpages Apprentissage|Test")
```


